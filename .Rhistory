obj_risk_binned = case_when(
automation_risk <= 2 ~ 1,
automation_risk <= 4 ~ 2,
automation_risk <= 6 ~ 3,
automation_risk <= 8 ~ 4,
automation_risk <= 10 ~ 5
),
# Calculate perception error (positive means overestimating risk)
risk_error = sh20a005 - obj_risk_binned,
# Create absolute error (for accuracy regardless of direction)
abs_risk_error = abs(risk_error),
# Education factor
education = factor(oplcat,
levels = 1:6,
labels = c("Primary", "VMBO", "HAVO/VWO", "MBO", "HBO", "WO"))
)
# Create summary by education level
education_summary <- analysis_data %>%
group_by(education) %>%
summarise(
# Sample size
n = n(),
# Raw scores (both now on 1-5 scale)
mean_subjective = mean(sh20a005, na.rm = TRUE),
mean_objective = mean(obj_risk_binned, na.rm = TRUE),
# Error metrics
mean_error = mean(risk_error, na.rm = TRUE),
sd_error = sd(risk_error, na.rm = TRUE),
# Percentage who overestimate
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
# Percentage who are exactly correct
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
# Percentage who underestimate
pct_underestimate = mean(risk_error < 0, na.rm = TRUE) * 100
) %>%
# Round all numeric columns to 2 decimal places
mutate(across(where(is.numeric), ~round(., 2)))
# Print the summary table
print("Summary Statistics by Education Level (both measures on 1-5 scale):")
print(education_summary)
# Reshape for grouped bar plot
plot_data <- education_summary %>%
select(education, mean_subjective, mean_objective) %>%
gather(key = "type", value = "risk", -education)
library(tidyr)
plot_data <- education_summary %>%
select(education, mean_subjective, mean_objective) %>%
gather(key = "type", value = "risk", -education)
plot_data <- education_summary %>%
dplyr::select(education, mean_subjective, mean_objective) %>%
gather(key = "type", value = "risk", -education)
ggplot(plot_data, aes(x = education, y = risk, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
coord_flip() +
labs(x = "Education Level",
y = "Risk Score (1-5 scale)",
title = "Risk Perception vs Binned Objective Risk by Education Level",
fill = "Risk Type") +
scale_fill_manual(values = c("mean_subjective" = "blue", "mean_objective" = "red"),
labels = c("Subjective", "Objective"))
library(ggplot2)
plot_data <- education_summary %>%
dplyr::select(education, mean_subjective, mean_objective) %>%
gather(key = "type", value = "risk", -education)
ggplot(plot_data, aes(x = education, y = risk, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
coord_flip() +
labs(x = "Education Level",
y = "Risk Score (1-5 scale)",
title = "Risk Perception vs Binned Objective Risk by Education Level",
fill = "Risk Type") +
scale_fill_manual(values = c("mean_subjective" = "blue", "mean_objective" = "red"),
labels = c("Subjective", "Objective"))
# Print the summary table
print("Summary Statistics by Education Level (both measures on 1-5 scale):")
print(education_summary)
# First, run ANOVA to test for overall differences between education levels
anova_result <- aov(risk_error ~ education, data = analysis_data)
# Run Tukey's HSD for pairwise comparisons
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)
# Calculate percentages with confidence intervals
library(broom)
library(dplyr)
education_summary <- analysis_data %>%
group_by(education) %>%
summarise(
n = n(),
# Overestimation
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
se_overestimate = sqrt((pct_overestimate/100 * (1-pct_overestimate/100)) / n) * 100,
ci_lower_over = pct_overestimate - 1.96 * se_overestimate,
ci_upper_over = pct_overestimate + 1.96 * se_overestimate,
# Correct estimation
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
se_correct = sqrt((pct_correct/100 * (1-pct_correct/100)) / n) * 100,
ci_lower_correct = pct_correct - 1.96 * se_correct,
ci_upper_correct = pct_correct + 1.96 * se_correct
)
# Create line plot with confidence intervals
plot_data <- education_summary %>%
select(education,
starts_with("pct_"),
starts_with("ci_lower_"),
starts_with("ci_upper_")) %>%
tidyr::pivot_longer(
cols = c(pct_overestimate, pct_correct),
names_to = "metric",
values_to = "percentage"
) %>%
mutate(
ci_lower = if_else(metric == "pct_overestimate",
ci_lower_over, ci_lower_correct),
ci_upper = if_else(metric == "pct_overestimate",
ci_upper_over, ci_upper_correct)
)
# Calculate percentages with confidence intervals
library(broom)
library(dplyr)
education_summary <- analysis_data %>%
group_by(education) %>%
summarise(
n = n(),
# Overestimation
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
se_overestimate = sqrt((pct_overestimate/100 * (1-pct_overestimate/100)) / n) * 100,
ci_lower_over = pct_overestimate - 1.96 * se_overestimate,
ci_upper_over = pct_overestimate + 1.96 * se_overestimate,
# Correct estimation
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
se_correct = sqrt((pct_correct/100 * (1-pct_correct/100)) / n) * 100,
ci_lower_correct = pct_correct - 1.96 * se_correct,
ci_upper_correct = pct_correct + 1.96 * se_correct
)
# Create line plot with confidence intervals
plot_data <- education_summary %>%
dplyr::select(education,
starts_with("pct_"),
starts_with("ci_lower_"),
starts_with("ci_upper_")) %>%
tidyr::pivot_longer(
cols = c(pct_overestimate, pct_correct),
names_to = "metric",
values_to = "percentage"
) %>%
mutate(
ci_lower = if_else(metric == "pct_overestimate",
ci_lower_over, ci_lower_correct),
ci_upper = if_else(metric == "pct_overestimate",
ci_upper_over, ci_upper_correct)
)
# Create the plot
ggplot(plot_data, aes(x = education, y = percentage,
color = metric, group = metric)) +
geom_line(size = 1) +
geom_point(size = 3) +
geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = metric),
alpha = 0.2, color = NA) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "Education Level",
y = "Percentage",
title = "Risk Assessment Accuracy by Education Level",
subtitle = "With 95% confidence intervals") +
scale_color_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation")) +
scale_fill_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation"))
# Print statistical test results
print("ANOVA Results - Testing for differences between education levels:")
print(summary(anova_result))
print("\nTukey's HSD Results - Pairwise comparisons:")
print(tukey_result)
#graph
education_summary <- analysis_data %>%
# Filter out NA education values
filter(!is.na(education)) %>%
group_by(education) %>%
summarise(
n = n(),
# Overestimation
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
se_overestimate = sqrt((pct_overestimate/100 * (1-pct_overestimate/100)) / n) * 100,
ci_lower_over = pct_overestimate - 1.96 * se_overestimate,
ci_upper_over = pct_overestimate + 1.96 * se_overestimate,
# Correct estimation
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
se_correct = sqrt((pct_correct/100 * (1-pct_correct/100)) / n) * 100,
ci_lower_correct = pct_correct - 1.96 * se_correct,
ci_upper_correct = pct_correct + 1.96 * se_correct
)
# Create line plot with confidence intervals
plot_data <- education_summary %>%
dplyr::select(education,
starts_with("pct_"),
starts_with("ci_lower_"),
starts_with("ci_upper_")) %>%
tidyr::pivot_longer(
cols = c(pct_overestimate, pct_correct),
names_to = "metric",
values_to = "percentage"
) %>%
mutate(
ci_lower = if_else(metric == "pct_overestimate",
ci_lower_over, ci_lower_correct),
ci_upper = if_else(metric == "pct_overestimate",
ci_upper_over, ci_upper_correct)
)
# Create the plot
ggplot(plot_data, aes(x = education, y = percentage,
color = metric, group = metric)) +
geom_line(size = 1) +
geom_point(size = 3) +
geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = metric),
alpha = 0.2, color = NA) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "Education Level",
y = "Percentage",
title = "Risk Assessment Accuracy by Education Level",
subtitle = "With 95% confidence intervals") +
scale_color_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation")) +
scale_fill_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation"))
# For statistical tests, also filter NA from original data
filtered_data <- analysis_data %>%
filter(!is.na(education))
# STATISTICAL TEST FOR EFFECT OF EDUCATION
# Ensure education and risk variables are properly formatted
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(perceived_risk, ordered = TRUE),
actual_risk = scale(actual_risk) # standardize for interpretation
)
# STATISTICAL TEST FOR EFFECT OF EDUCATION
# Ensure education and risk variables are properly formatted
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(perceived_risk, ordered = TRUE),
actual_risk = scale(actual_risk) # standardize for interpretation
)
rlang::last_trace()
# Create summary by education level
education_summary <- analysis_data %>%
group_by(education) %>%
summarise(
# Sample size
n = n(),
# Raw scores (both now on 1-5 scale)
mean_subjective = mean(sh20a005, na.rm = TRUE),
mean_objective = mean(obj_risk_binned, na.rm = TRUE),
# Error metrics
mean_error = mean(risk_error, na.rm = TRUE),
sd_error = sd(risk_error, na.rm = TRUE),
# Percentage who overestimate
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
# Percentage who are exactly correct
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
# Percentage who underestimate
pct_underestimate = mean(risk_error < 0, na.rm = TRUE) * 100
) %>%
# Round all numeric columns to 2 decimal places
mutate(across(where(is.numeric), ~round(., 2)))
# Print the summary table
print("Summary Statistics by Education Level (both measures on 1-5 scale):")
print(education_summary)
# First, run ANOVA to test for overall differences between education levels
anova_result <- aov(risk_error ~ education, data = analysis_data)
# Run Tukey's HSD for pairwise comparisons
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)
#GRAPH!!
education_summary <- analysis_data %>%
# Filter out NA education values
filter(!is.na(education)) %>%
group_by(education) %>%
summarise(
n = n(),
# Overestimation
pct_overestimate = mean(risk_error > 0, na.rm = TRUE) * 100,
se_overestimate = sqrt((pct_overestimate/100 * (1-pct_overestimate/100)) / n) * 100,
ci_lower_over = pct_overestimate - 1.96 * se_overestimate,
ci_upper_over = pct_overestimate + 1.96 * se_overestimate,
# Correct estimation
pct_correct = mean(risk_error == 0, na.rm = TRUE) * 100,
se_correct = sqrt((pct_correct/100 * (1-pct_correct/100)) / n) * 100,
ci_lower_correct = pct_correct - 1.96 * se_correct,
ci_upper_correct = pct_correct + 1.96 * se_correct
)
# Create line plot with confidence intervals
plot_data <- education_summary %>%
dplyr::select(education,
starts_with("pct_"),
starts_with("ci_lower_"),
starts_with("ci_upper_")) %>%
tidyr::pivot_longer(
cols = c(pct_overestimate, pct_correct),
names_to = "metric",
values_to = "percentage"
) %>%
mutate(
ci_lower = if_else(metric == "pct_overestimate",
ci_lower_over, ci_lower_correct),
ci_upper = if_else(metric == "pct_overestimate",
ci_upper_over, ci_upper_correct)
)
# Create the plot
ggplot(plot_data, aes(x = education, y = percentage,
color = metric, group = metric)) +
geom_line(size = 1) +
geom_point(size = 3) +
geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = metric),
alpha = 0.2, color = NA) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "Education Level",
y = "Percentage",
title = "Risk Assessment Accuracy by Education Level",
subtitle = "With 95% confidence intervals") +
scale_color_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation")) +
scale_fill_manual(values = c("pct_correct" = "blue",
"pct_overestimate" = "red"),
labels = c("Correct Assessment",
"Overestimation"))
# For statistical tests, also filter NA from original data
filtered_data <- analysis_data %>%
filter(!is.na(education))
# STATISTICAL TEST FOR EFFECT OF EDUCATION
# Ensure education and risk variables are properly formatted
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(perceived_risk, ordered = TRUE),
actual_risk = scale(actual_risk) # standardize for interpretation
)
# Fit ordered logistic regression with interaction
mod_interaction <- polr(
perceived_risk ~ actual_risk * education,
data = analysis_data,
Hess = TRUE
)
# Ensure education and risk variables are properly formatted
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(sh20a005, ordered = TRUE),
actual_risk = scale(actual_risk) # standardize for interpretation
)
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(sh20a005, ordered = TRUE)
)
# Fit ordered logistic regression with interaction
mod_interaction <- polr(
perceived_risk ~ actual_risk * education,
data = analysis_data,
Hess = TRUE
)
# STATISTICAL TEST FOR EFFECT OF EDUCATION
# Ensure education and risk variables are properly formatted
analysis_data <- analysis_data %>%
filter(!is.na(education)) %>%
mutate(
education = factor(education, ordered = TRUE),
perceived_risk = factor(sh20a005, ordered = TRUE)
)
# Fit ordered logistic regression with interaction
mod_interaction <- polr(
perceived_risk ~ obj_risk_binned * education,
data = analysis_data,
Hess = TRUE
)
# Get model summary
summary_table <- tidy(mod_interaction)
# Calculate significance levels
mod_ctable <- coef(summary(mod_interaction))
p_values <- pnorm(abs(mod_ctable[, "t value"]), lower.tail = FALSE) * 2
summary_with_p <- cbind(summary_table, p_value = p_values)
# Likelihood ratio test for interaction terms
mod_no_interaction <- polr(
perceived_risk ~ obj_risk_binned + education,
data = analysis_data,
Hess = TRUE
)
lr_test <- anova(mod_no_interaction, mod_interaction)
# Print results
print("Ordered Logistic Regression Results:")
print(summary_with_p)
print("\nLikelihood Ratio Test for Interaction Terms:")
print(lr_test)
# Prepare data
analysis_data <- final_clean_data %>%
mutate(
# Create standardized version of objective risk for easier interpretation
automation_risk_std = scale(automation_risk),
# Keep subjective risk as ordered factor (1-5)
subjective_risk = factor(sh20a005,
levels = 1:5,
ordered = TRUE),
# Controls
education = factor(oplcat,
levels = 1:6,
labels = c("Primary", "VMBO", "HAVO/VWO", "MBO", "HBO", "WO")),
female = as.factor(geslacht == 2)
)
# Ordered logistic regression since subjective risk is ordinal (1-5)
library(MASS)
# Base model: just objective risk
model1 <- polr(subjective_risk ~ automation_risk_std,
data = analysis_data,
Hess = TRUE)
# Full model with controls
model2 <- polr(subjective_risk ~ automation_risk_std +
education + age_group + female,
data = analysis_data,
Hess = TRUE)
#STAT TEST
# Prepare data
analysis_data <- final_clean_data %>%
mutate(
# Create standardized version of objective risk for easier interpretation
automation_risk_std = scale(automation_risk),
# Keep subjective risk as ordered factor (1-5)
subjective_risk = factor(sh20a005,
levels = 1:5,
ordered = TRUE),
# Controls
education = factor(oplcat,
levels = 1:6,
labels = c("Primary", "VMBO", "HAVO/VWO", "MBO", "HBO", "WO")),
female = as.factor(geslacht == 2)
)
# Ordered logistic regression since subjective risk is ordinal (1-5)
library(MASS)
# Base model: just objective risk
model1 <- polr(subjective_risk ~ automation_risk_std,
data = analysis_data,
Hess = TRUE)
# Full model with controls
model2 <- polr(subjective_risk ~ automation_risk_std +
education,
data = analysis_data,
Hess = TRUE)
# Calculate odds ratios and CIs
get_or_ci <- function(model) {
ctable <- coef(summary(model))
or <- exp(ctable[, "Value"])
ci <- exp(cbind(ctable[, "Value"] - 1.96 * ctable[, "Std. Error"],
ctable[, "Value"] + 1.96 * ctable[, "Std. Error"]))
return(data.frame(OR = or,
CI_lower = ci[,1],
CI_upper = ci[,2]))
}
# Print results
print("Model 1 - Basic relationship:")
summary(model1)
print("\nOdds ratios and 95% CI:")
print(get_or_ci(model1))
print("\nModel 2 - With controls:")
summary(model2)
print("\nOdds ratios and 95% CI:")
print(get_or_ci(model2))
# Base model: just objective risk
model1 <- polr(subjective_risk ~ automation_risk,
data = analysis_data,
Hess = TRUE)
# Full model with controls
model2 <- polr(subjective_risk ~ automation_risk +
education,
data = analysis_data,
Hess = TRUE)
# Calculate odds ratios and CIs
get_or_ci <- function(model) {
ctable <- coef(summary(model))
or <- exp(ctable[, "Value"])
ci <- exp(cbind(ctable[, "Value"] - 1.96 * ctable[, "Std. Error"],
ctable[, "Value"] + 1.96 * ctable[, "Std. Error"]))
return(data.frame(OR = or,
CI_lower = ci[,1],
CI_upper = ci[,2]))
}
# Print results
print("Model 1 - Basic relationship:")
summary(model1)
print("\nOdds ratios and 95% CI:")
print(get_or_ci(model1))
print("\nModel 2 - With controls:")
summary(model2)
print("\nOdds ratios and 95% CI:")
print(get_or_ci(model2))
